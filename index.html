
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>The Silicon Scroll | Weekly Research</title>
        <style>
            @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@900&family=Libre+Baskerville:wght@400;700&display=swap');
            
            body { background: #f4f1ea; color: #1a1a1a; font-family: 'Libre Baskerville', serif; margin: 0; padding: 2vw; }
            .masthead { text-align: center; border-bottom: 5px double #333; margin-bottom: 40px; padding-bottom: 15px; }
            .masthead h1 { font-family: 'Playfair Display'; font-size: 5rem; margin: 0; letter-spacing: -2px; }
            
            .news-desk { margin-bottom: 50px; }
            .desk-title { 
                border-bottom: 2px solid #333; 
                font-family: 'Playfair Display'; 
                font-size: 1.8rem; 
                margin-bottom: 15px; 
                padding-bottom: 5px; 
                display: flex; 
                justify-content: space-between; 
                align-items: center; 
            }
            .desk-title span { font-size: 0.7rem; font-family: 'Libre Baskerville'; opacity: 0.5; }
            
            .horizontal-scroll { 
                display: flex; 
                overflow-x: auto; 
                gap: 25px; 
                padding-bottom: 20px;
                scrollbar-width: thin;
                scrollbar-color: #333 #f4f1ea;
            }
            
            .horizontal-scroll::-webkit-scrollbar { height: 8px; }
            .horizontal-scroll::-webkit-scrollbar-thumb { background: #333; border-radius: 4px; }

            .card { 
                flex: 0 0 350px; 
                background: #fffefc; 
                border: 1px solid #d1cec1; 
                padding: 25px; 
                box-shadow: 4px 4px 0px rgba(0,0,0,0.05);
                transition: transform 0.2s;
                display: flex;
                flex-direction: column;
            }
            .card:hover { transform: translateY(-5px); }
            
            .card-date { font-size: 10px; font-weight: bold; color: #777; margin-bottom: 10px; }
            h3 { font-family: 'Playfair Display'; font-size: 1.3rem; margin: 0 0 10px 0; line-height: 1.2; }
            h3 a { color: #1a1a1a; text-decoration: none; }
            .meta { font-size: 11px; font-weight: bold; text-transform: uppercase; color: #555; margin-bottom: 10px; display: block; }
            .text { font-size: 13px; line-height: 1.6; color: #333; flex-grow: 1; }

            .ai-accent { border-top: 6px solid #2c3e50; }
            .nlp-accent { border-top: 6px solid #27ae60; }
            .vision-accent { border-top: 6px solid #e67e22; }
        </style>
    </head>
    <body>
        <div class="masthead">
            <div style="font-size: 50px; margin-bottom: 10px;">ðŸ¦‰</div>
            <h1>The Silicon Scroll</h1>
            <p>TEMPE, AZ â€” FEBRUARY 24, 2026 â€” WEEKLY INTELLIGENCE</p>
        </div>
        
        <section class="news-desk">
            <h2 class="desk-title">AI & REINFORCEMENT DESK <span>231 PAPERS THIS WEEK</span></h2>
            <div class="horizontal-scroll">
                
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20078v1" target="_blank">ðŸ”¥ Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning</a></h3>
                <p class="meta">By Shan Yang, Yang Liu</p>
                <p class="text">Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradien...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20064v1" target="_blank">ðŸ”¥ The LLMbda Calculus: AI Agents, Conversations, and Information Flow</a></h3>
                <p class="meta">By Zac Garby, Andrew D. Gordon</p>
                <p class="text">A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool i...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20057v1" target="_blank">ðŸ”¥ AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation</a></h3>
                <p class="meta">By Ge Yuan, Qiyuan Qiao</p>
                <p class="text">Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a u...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19843v1" target="_blank">ðŸ”¥ MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems</a></h3>
                <p class="meta">By Jin Jia, Zhiling Deng</p>
                <p class="text">As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallu...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19634v1" target="_blank">ðŸ”¥ Compositional Planning with Jumpy World Models</a></h3>
                <p class="meta">By Jesse Farebrother, Matteo Pirotta</p>
                <p class="text">The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone ...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19538v1" target="_blank">ðŸ”¥ Cost-Aware Diffusion Active Search</a></h3>
                <p class="meta">By Arundhati Banerjee, Jeff Schneider</p>
                <p class="text">Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thom...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19326v1" target="_blank">ðŸ”¥ City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification</a></h3>
                <p class="meta">By Rui Liu, Steven Jige Quan</p>
                <p class="text">As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual e...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19304v1" target="_blank">ðŸ”¥ Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation</a></h3>
                <p class="meta">By Haojun Shi, Suyu Ye</p>
                <p class="text">Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In ...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19223v1" target="_blank">ðŸ”¥ Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment</a></h3>
                <p class="meta">By Aymen Khouja, Imen Jendoubi</p>
                <p class="text">The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MA...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19128v1" target="_blank">ðŸ”¥ K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model</a></h3>
                <p class="meta">By Shiyi Cao, Ziming Mao</p>
                <p class="text">Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic co...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19065v1" target="_blank">ðŸ”¥ Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents</a></h3>
                <p class="meta">By Chanjin Park</p>
                <p class="text">Large Language Models (LLMs) are evolving into autonomous agents, yet current "frameless" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability,...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19000v1" target="_blank">ðŸ”¥ MagicAgent: Towards Generalized Agent Planning</a></h3>
                <p class="meta">By Xuhui Ren, Shaokang Dong</p>
                <p class="text">The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data ...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.18986v1" target="_blank">ðŸ”¥ Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight</a></h3>
                <p class="meta">By Vishal Srivastava, Tanmay Sah</p>
                <p class="text">Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a par...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18968v1" target="_blank">ðŸ”¥ Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction</a></h3>
                <p class="meta">By Tao Zhe, Haoyu Wang</p>
                <p class="text">Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit plannin...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18940v1" target="_blank">ðŸ”¥ DREAM: Deep Research Evaluation with Agentic Metrics</a></h3>
                <p class="meta">By Elad Ben Avraham, Changhao Li</p>
                <p class="text">Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synth...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18916v1" target="_blank">ðŸ”¥ Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning</a></h3>
                <p class="meta">By Hoang-Loc Cao, Phuc Ho</p>
                <p class="text">Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce ...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18824v1" target="_blank">ðŸ”¥ UniRank: A Multi-Agent Calibration Pipeline for Estimating University Rankings from Anonymized Bibliometric Signals</a></h3>
                <p class="meta">By Pedram Riyazimehr, Seyyed Ehsan Mahmoudi</p>
                <p class="text">We present UniRank, a multi-agent LLM pipeline that estimates university positions across global ranking systems using only publicly available bibliometric data from OpenAlex and Semantic Scholar. The system employs a three-stage architecture: (a) zero-shot estimation from anonym...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18797v1" target="_blank">ðŸ”¥ Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning</a></h3>
                <p class="meta">By Mubshra Zulfiqar, Muhammad Ayzed Mirza</p>
                <p class="text">Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-us...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18740v1" target="_blank">ðŸ”¥ HONEST-CAV: Hierarchical Optimization of Network Signals and Trajectories for Connected and Automated Vehicles with Multi-Agent Reinforcement Learning</a></h3>
                <p class="meta">By Ziyan Zhang, Changxin Wan</p>
                <p class="text">This study presents a hierarchical, network-level traffic flow control framework for mixed traffic consisting of Human-driven Vehicles (HVs), Connected and Automated Vehicles (CAVs). The framework jointly optimizes vehicle-level eco-driving behaviors and intersection-level traffi...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18731v1" target="_blank">ðŸ”¥ Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization</a></h3>
                <p class="meta">By Yuhang Bai, Yujuan Ding</p>
                <p class="text">Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the dee...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18650v1" target="_blank">ðŸ”¥ NutriOrion: A Hierarchical Multi-Agent Framework for Personalized Nutrition Intervention Grounded in Clinical Guidelines</a></h3>
                <p class="meta">By Junwei Wu, Runze Yan</p>
                <p class="text">Personalized nutrition intervention for patients with multimorbidity is critical for improving health outcomes, yet remains challenging because it requires the simultaneous integration of heterogeneous clinical conditions, medications, and dietary guidelines. Single-agent large l...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18640v1" target="_blank">ðŸ”¥ Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System</a></h3>
                <p class="meta">By Longfei Yun, Yihan Wu</p>
                <p class="text">Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of tran...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20144v1" target="_blank">ðŸ”¥ Agentic AI for Scalable and Robust Optical Systems Control</a></h3>
                <p class="meta">By Zehao Wang, Mingzhe Han</p>
                <p class="text">We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structu...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20059v1" target="_blank">ðŸ”¥ Interaction Theater: A case of LLM Agents Interacting at Scale</a></h3>
                <p class="meta">By Sarath Shekkizhar, Adam Earle</p>
                <p class="text">As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20048v1" target="_blank">ðŸ”¥ CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence</a></h3>
                <p class="meta">By Tarakanath Paipuru</p>
                <p class="text">Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigati...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19810v1" target="_blank">ðŸ”¥ OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research</a></h3>
                <p class="meta">By Lukas Weidener, Marko BrkiÄ‡</p>
                <p class="text">In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19555v1" target="_blank">ðŸ”¥ Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains</a></h3>
                <p class="meta">By Xiaochong Jiang, Shiqi Yang</p>
                <p class="text">Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulat...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19502v1" target="_blank">ðŸ”¥ Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark</a></h3>
                <p class="meta">By Lalitha Pranathi Pulavarthy, Raajitha Muthyala</p>
                <p class="text">Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19458v1" target="_blank">ðŸ”¥ ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making</a></h3>
                <p class="meta">By Ziyang Guo, Yifan Wu</p>
                <p class="text">Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a deci...</p>
            </div>
            
            <div class="card ai-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18971v1" target="_blank">ðŸ”¥ When Do LLM Preferences Predict Downstream Behavior?</a></h3>
                <p class="meta">By Katarina Slama, Alexandra Souly</p>
                <p class="text">Preference-driven behavior in LLMs may be a necessary precondition for AI misalignment such as sandbagging: models cannot strategically pursue misaligned goals unless their behavior is influenced by their preferences. Yet prior work has typically prompted models explicitly to act...</p>
            </div>
            
            </div>
        </section>
        
        <section class="news-desk">
            <h2 class="desk-title">NLP & LANGUAGE DESK <span>68 PAPERS THIS WEEK</span></h2>
            <div class="horizontal-scroll">
                
            <div class="card nlp-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19320v1" target="_blank">ðŸ”¥ Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations</a></h3>
                <p class="meta">By Dongming Jiang, Yi Li</p>
                <p class="text">Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems rema...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19127v1" target="_blank">ðŸ”¥ AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG</a></h3>
                <p class="meta">By Qijie You, Wenkai Yu</p>
                <p class="text">With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessin...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20040v1" target="_blank">ðŸ”¥ AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization</a></h3>
                <p class="meta">By Fahmida Liza Piya, Rahmatollah Beheshti</p>
                <p class="text">Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic fram...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19961v1" target="_blank">ðŸ”¥ Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval</a></h3>
                <p class="meta">By Yibo Yan, Jiahao Huo</p>
                <p class="text">With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19840v1" target="_blank">ðŸ”¥ SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation</a></h3>
                <p class="meta">By Jingzhuo Wu, Jiajun Zhang</p>
                <p class="text">Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-mod...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18966v1" target="_blank">ðŸ”¥ Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation</a></h3>
                <p class="meta">By Yonathan Ron, Shiri Gilboa</p>
                <p class="text">Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcript...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18922v1" target="_blank">ðŸ”¥ Why Agent Caching Fails and How to Fix It: Structured Intent Canonicalization with Few-Shot Learning</a></h3>
                <p class="meta">By Abhinaba Basu</p>
                <p class="text">Personal AI agents incur substantial cost via repeated LLM calls. We show existing caching methods fail: GPTCache achieves 37.9% accuracy on real benchmarks; APC achieves 0-12%. The root cause is optimizing for the wrong property -- cache effectiveness requires key consistency an...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18734v1" target="_blank">ðŸ”¥ Rethinking Retrieval-Augmented Generation as a Cooperative Decision-Making Problem</a></h3>
                <p class="meta">By Lichang Song, Ting Long</p>
                <p class="text">Retrieval-Augmented Generation (RAG) has demonstrated strong effectiveness in knowledge-intensive tasks by grounding language generation in external evidence. Despite its success, many existing RAG systems are built based on a ranking-centric, asymmetric dependency paradigm, wher...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18425v1" target="_blank">ðŸ”¥ RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering</a></h3>
                <p class="meta">By Deniz Qian, Hung-Ting Chen</p>
                <p class="text">Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original quer...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20130v1" target="_blank">âš¡ To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering</a></h3>
                <p class="meta">By Zaifu Zhan, Min Zeng</p>
                <p class="text">Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.   Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predi...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20017v1" target="_blank">âš¡ QUIETT: Query-Independent Table Transformation for Robust Reasoning</a></h3>
                <p class="meta">By Gaurav Najpande, Tampu Ravi Kumar</p>
                <p class="text">Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entang...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19948v1" target="_blank">âš¡ Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming</a></h3>
                <p class="meta">By Ian Steenstra, Paola Pedrelli</p>
                <p class="text">Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with sim...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19919v1" target="_blank">âš¡ Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling</a></h3>
                <p class="meta">By Xiang Li, Zikai Wei</p>
                <p class="text">Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual infor...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19883v1" target="_blank">âš¡ Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection</a></h3>
                <p class="meta">By Daham Mustafa, Diego Collarana</p>
                <p class="text">ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics ...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19878v1" target="_blank">âš¡ Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics</a></h3>
                <p class="meta">By Daham Mustafa, Diego Collarana</p>
                <p class="text">Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text expli...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19569v1" target="_blank">âš¡ Temporal-Aware Heterogeneous Graph Reasoning with Multi-View Fusion for Temporal Question Answering</a></h3>
                <p class="meta">By Wuzhenghong Wen, Bowen Zhou</p>
                <p class="text">Question Answering over Temporal Knowledge Graphs (TKGQA) has attracted growing interest for handling time-sensitive queries. However, existing methods still struggle with: 1) weak incorporation of temporal constraints in question representation, causing biased reasoning; 2) limi...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19526v1" target="_blank">âš¡ How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1</a></h3>
                <p class="meta">By Yinuo Xu, Shuo Lu</p>
                <p class="text">Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of ...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19509v1" target="_blank">âš¡ Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference</a></h3>
                <p class="meta">By Arindam Khaled</p>
                <p class="text">Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While "Oracle" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters)...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19317v1" target="_blank">âš¡ Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering</a></h3>
                <p class="meta">By Maryam Amirizaniani, Alireza Salemi</p>
                <p class="text">Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal c...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19212v1" target="_blank">âš¡ Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection</a></h3>
                <p class="meta">By Raihan Tanvir, Md. Golam Rabiul Alam</p>
                <p class="text">Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive cod...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19049v1" target="_blank">âš¡ IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning</a></h3>
                <p class="meta">By Yinhan He, Yaochen Zhu</p>
                <p class="text">Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how ...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18806v1" target="_blank">âš¡ Think$^{2}$: Grounded Metacognitive Reasoning in Large Language Models</a></h3>
                <p class="meta">By Abraham Paul Elenjical, Vivek Hruday Kavuri</p>
                <p class="text">Large Language Models (LLMs) demonstrate strong reasoning performance, yet their ability to reliably monitor, diagnose, and correct their own errors remains limited. We introduce a psychologically grounded metacognitive framework that operationalizes Ann Brown's regulatory cycle ...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18788v1" target="_blank">âš¡ BURMESE-SAN: Burmese NLP Benchmark for Evaluating Large Language Models</a></h3>
                <p class="meta">By Thura Aung, Jann Railey Montalan</p>
                <p class="text">We introduce BURMESE-SAN, the first holistic benchmark that systematically evaluates large language models (LLMs) for Burmese across three core NLP competencies: understanding (NLU), reasoning (NLR), and generation (NLG). BURMESE-SAN consolidates seven subtasks spanning these com...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18693v1" target="_blank">âš¡ Contradiction to Consensus: Dual Perspective, Multi Source Retrieval Based Claim Verification with Source Level Disagreement using LLM</a></h3>
                <p class="meta">By Md Badsha Biswas, Ozlem Uzuner</p>
                <p class="text">The spread of misinformation across digital platforms can pose significant societal risks. Claim verification, a.k.a. fact-checking, systems can help identify potential misinformation. However, their efficacy is limited by the knowledge sources that they rely on. Most automated c...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18633v1" target="_blank">âš¡ DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-Tuning</a></h3>
                <p class="meta">By Fangyuan Xu, Sihao Chen</p>
                <p class="text">Differentially private (DP) synthetic data generation plays a pivotal role in developing large language models (LLMs) on private data, where data owners cannot provide eyes-on access to individual examples. Generating DP synthetic data typically involves a difficult trade-off. On...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18429v1" target="_blank">âš¡ VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning</a></h3>
                <p class="meta">By Harshul Raj Surana, Arijit Maji</p>
                <p class="text">Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving ...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20135v1" target="_blank">KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration</a></h3>
                <p class="meta">By Mohammad Amanlou, Erfan Shafiee Moghaddam</p>
                <p class="text">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an L...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20122v1" target="_blank">NanoKnow: How to Know What Your Language Model Knows</a></h3>
                <p class="meta">By Lingwei Gu, Nour Jedidi</p>
                <p class="text">How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a "black box" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20092v1" target="_blank">BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop</a></h3>
                <p class="meta">By Leshem Choshen, Ryan Cotterell</p>
                <p class="text">BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the gene...</p>
            </div>
            
            <div class="card nlp-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20091v1" target="_blank">How Retrieved Context Shapes Internal Representations in RAG</a></h3>
                <p class="meta">By Samuel Yeh, Sharon Li</p>
                <p class="text">Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of...</p>
            </div>
            
            </div>
        </section>
        
        <section class="news-desk">
            <h2 class="desk-title">VISION & MULTIMODAL DESK <span>201 PAPERS THIS WEEK</span></h2>
            <div class="horizontal-scroll">
                
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19571v1" target="_blank">ðŸ”¥ HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies</a></h3>
                <p class="meta">By Chang Liu, Yunfan Ye</p>
                <p class="text">Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separate...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19442v1" target="_blank">ðŸ”¥ UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment</a></h3>
                <p class="meta">By Yecheng Zhang, Rong Zhao</p>
                <p class="text">Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved ...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-21</div>
                <h3><a href="http://arxiv.org/abs/2602.18887v1" target="_blank">ðŸ”¥ SafeDrive: Fine-Grained Safety Reasoning for End-to-End Driving in a Sparse World</a></h3>
                <p class="meta">By Jungho Kim, Jiyong Oh</p>
                <p class="text">The end-to-end (E2E) paradigm, which maps sensor inputs directly to driving decisions, has recently attracted significant attention due to its unified modeling capability and scalability. However, ensuring safety in this unified framework remains one of the most critical challeng...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19570v1" target="_blank">ðŸ”¥ VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense</a></h3>
                <p class="meta">By Nadav Kadvil, Ayellet Tal</p>
                <p class="text">Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation t...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19542v1" target="_blank">ðŸ”¥ Vinedresser3D: Agentic Text-guided 3D Editing</a></h3>
                <p class="meta">By Yankuan Chi, Xiang Li</p>
                <p class="text">Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for hi...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19503v1" target="_blank">ðŸ”¥ A Text-Guided Vision Model for Enhanced Recognition of Small Instances</a></h3>
                <p class="meta">By Hyun-Ki Jung</p>
                <p class="text">As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To addre...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19285v1" target="_blank">ðŸ”¥ MRI Contrast Enhancement Kinetics World Model</a></h3>
                <p class="meta">By Jindi Kong, Yuting He</p>
                <p class="text">Clinical MRI contrast acquisition suffers from inefficient information yield, which presents as a mismatch between the risky and costly acquisition protocol and the fixed and sparse acquisition sequence. Applying world models to simulate the contrast enhancement kinetics in the h...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18432v1" target="_blank">ðŸ”¥ SARAH: Spatially Aware Real-time Agentic Humans</a></h3>
                <p class="meta">By Evonne Ng, Siwei Zhang</p>
                <p class="text">As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-20</div>
                <h3><a href="http://arxiv.org/abs/2602.18422v1" target="_blank">ðŸ”¥ Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control</a></h3>
                <p class="meta">By Linxi Xie, Lisong C. Sun</p>
                <p class="text">Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video ...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20159v1" target="_blank">âš¡ A Very Big Video Reasoning Suite</a></h3>
                <p class="meta">By Maijunxian Wang, Ruisi Wang</p>
                <p class="text">Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive rea...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20066v1" target="_blank">âš¡ HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images</a></h3>
                <p class="meta">By Kundan Thota, Xuanhao Mu</p>
                <p class="text">Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.20060v1" target="_blank">âš¡ MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving</a></h3>
                <p class="meta">By Junli Wang, Xueyi Liu</p>
                <p class="text">Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vo...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19974v1" target="_blank">âš¡ RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection</a></h3>
                <p class="meta">By Tianyu Wang, Zhiyuan Ma</p>
                <p class="text">Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relations...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19828v1" target="_blank">âš¡ TextShield-R1: Reinforced Reasoning for Tampered Text Detection</a></h3>
                <p class="meta">By Chenfan Qu, Yiwu Zhong</p>
                <p class="text">The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they ...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19768v1" target="_blank">âš¡ TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding</a></h3>
                <p class="meta">By Fan Yang, Shurong Zheng</p>
                <p class="text">Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and ex...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19715v1" target="_blank">âš¡ Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision</a></h3>
                <p class="meta">By Kartik Kuckreja, Parul Gupta</p>
                <p class="text">Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framewo...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19697v1" target="_blank">âš¡ BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU</a></h3>
                <p class="meta">By Soumya Mazumdar, Vineet Kumar Rakesh</p>
                <p class="text">Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however,...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19679v1" target="_blank">âš¡ TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures</a></h3>
                <p class="meta">By Hyeongjin Nam, Daniel Sungho Jung</p>
                <p class="text">Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely ...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19668v1" target="_blank">âš¡ Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation</a></h3>
                <p class="meta">By He Zhu, Ren Togo</p>
                <p class="text">Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods large...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19615v1" target="_blank">âš¡ Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness</a></h3>
                <p class="meta">By Xin Hu, Haomiao Ni</p>
                <p class="text">Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving a...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19505v1" target="_blank">âš¡ Test-Time Computing for Referring Multimodal Large Language Models</a></h3>
                <p class="meta">By Mingrui Wu, Hao Chen</p>
                <p class="text">We propose ControlMLLM++, a novel test-time adaptation framework that injects learnable visual prompts into frozen multimodal large language models (MLLMs) to enable fine-grained region-based visual reasoning without any model retraining or fine-tuning. Leveraging the insight tha...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19497v1" target="_blank">âš¡ MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models</a></h3>
                <p class="meta">By Mingrui Wu, Hang Liu</p>
                <p class="text">Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address th...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-23</div>
                <h3><a href="http://arxiv.org/abs/2602.19449v1" target="_blank">âš¡ Decoupling Vision and Language: Codebook Anchored Visual Adaptation</a></h3>
                <p class="meta">By Jason Wu, Tianchen Zhao</p>
                <p class="text">Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representati...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19357v1" target="_blank">âš¡ MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations</a></h3>
                <p class="meta">By Nilay Yilmaz, Maitreya Patel</p>
                <p class="text">Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art V...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19188v1" target="_blank">âš¡ PositionOCR: Augmenting Positional Awareness in Multi-Modal Models via Hybrid Specialist Integration</a></h3>
                <p class="meta">By Chen Duan, Zhentao Guo</p>
                <p class="text">In recent years, Multi-modal Large Language Models (MLLMs) have achieved strong performance in OCR-centric Visual Question Answering (VQA) tasks, illustrating their capability to process heterogeneous data and exhibit adaptability across varied contexts. However, these MLLMs rely...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19178v1" target="_blank">âš¡ EMAD: Evidence-Centric Grounded Multimodal Diagnosis for Alzheimer's Disease</a></h3>
                <p class="meta">By Qiuhui Chen, Xuancheng Yao</p>
                <p class="text">Deep learning models for medical image analysis often act as black boxes, seldom aligning with clinical guidelines or explicitly linking decisions to supporting evidence. This is especially critical in Alzheimer's disease (AD), where predictions should be grounded in both anatomi...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19146v1" target="_blank">âš¡ VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval</a></h3>
                <p class="meta">By Diogo GlÃ³ria-Silva, David Semedo</p>
                <p class="text">We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, pl...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19123v1" target="_blank">âš¡ StreetTree: A Large-Scale Global Benchmark for Fine-Grained Tree Species Classification</a></h3>
                <p class="meta">By Jiapeng Li, Yingjing Huang</p>
                <p class="text">The fine-grained classification of street trees is a crucial task for urban planning, streetscape management, and the assessment of urban ecosystem services. However, progress in this field has been significantly hindered by the lack of large-scale, geographically diverse, and pu...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19117v1" target="_blank">âš¡ Keep it SymPL: Symbolic Projective Layout for Allocentric Spatial Reasoning in Vision-Language Models</a></h3>
                <p class="meta">By Jaeyun Jang, Seunghui Shin</p>
                <p class="text">Perspective-aware spatial reasoning involves understanding spatial relationships from specific viewpoints-either egocentric (observer-centered) or allocentric (object-centered). While vision-language models (VLMs) perform well in egocentric settings, their performance deteriorate...</p>
            </div>
            
            <div class="card vision-accent">
                <div class="card-date">2026-02-22</div>
                <h3><a href="http://arxiv.org/abs/2602.19063v1" target="_blank">âš¡ Direction-aware 3D Large Multimodal Models</a></h3>
                <p class="meta">By Quan Liu, Weihao Xuan</p>
                <p class="text">3D large multimodal models (3D LMMs) rely heavily on ego poses for enabling directional question-answering and spatial reasoning. However, most existing point cloud benchmarks contain rich directional queries but lack the corresponding ego poses, making them inherently ill-posed ...</p>
            </div>
            
            </div>
        </section>
        
    </body>
    </html>
    